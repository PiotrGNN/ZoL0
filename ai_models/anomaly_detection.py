"""
anomaly_detection.py
-------------------
Modu≈Ç do wykrywania anomalii rynkowych za pomocƒÖ technik statystycznych i ML.

Funkcjonalno≈õci:
- Detekcja nietypowych wzorc√≥w cenowych
- Identyfikacja odstajƒÖcych wolumen√≥w
- Wykrywanie manipulacji rynkowych
- Alerty w czasie rzeczywistym
"""

import logging
import os
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# Konfiguracja loggera
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("anomaly_detection.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class AnomalyDetector:
    """Klasa implementujƒÖca wykrywanie anomalii rynkowych."""

    def __init__(self, sensitivity: float = 0.05, window_size: int = 100):
        """
        Inicjalizuje detektor anomalii.

        Args:
            sensitivity: Czu≈Ço≈õƒá detektora (0.01-0.1)
            window_size: Rozmiar okna analizy
        """
        self.sensitivity = sensitivity
        self.window_size = window_size
        self.scaler = StandardScaler()
        self.model = IsolationForest(
            contamination=self.sensitivity,
            random_state=42,
            n_jobs=-1
        )
        self.is_trained = False
        logger.info(f"‚úÖ Inicjalizacja detektora anomalii (czu≈Ço≈õƒá: {sensitivity})")

    @staticmethod
    def check_dependencies() -> bool:
        """Sprawdza, czy wszystkie zale≈ºno≈õci sƒÖ dostƒôpne."""
        try:
            import numpy
            import pandas
            import sklearn
            return True
        except ImportError:
            return False

    def train(self, data: pd.DataFrame) -> None:
        """
        Trenuje model na danych historycznych.

        Args:
            data: DataFrame z danymi historycznymi
        """
        try:
            if data.empty or len(data) < self.window_size:
                logger.warning("‚ö†Ô∏è Za ma≈Ço danych do treningu detektora anomalii")
                return

            features = self._extract_features(data)
            scaled_features = self.scaler.fit_transform(features)

            logger.info(f"üß† Trening modelu na {len(features)} pr√≥bkach")
            self.model.fit(scaled_features)
            self.is_trained = True
            logger.info("‚úÖ Model detektora anomalii wytrenowany")
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas treningu detektora anomalii: {e}")
            self.is_trained = False

    def detect(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Wykrywa anomalie w danych.

        Args:
            data: DataFrame z danymi do analizy

        Returns:
            DataFrame z flagami anomalii
        """
        if not self.is_trained:
            logger.warning("‚ö†Ô∏è Model nie jest wytrenowany. Uruchom train() najpierw.")
            return pd.DataFrame()

        try:
            features = self._extract_features(data)
            scaled_features = self.scaler.transform(features)

            # -1 oznacza anomaliƒô, 1 oznacza normalnƒÖ pr√≥bkƒô
            predictions = self.model.predict(scaled_features)
            scores = self.model.decision_function(scaled_features)

            # Dodajemy wyniki detekcji do oryginalnych danych
            result = data.copy()
            result['is_anomaly'] = [1 if pred == -1 else 0 for pred in predictions]
            result['anomaly_score'] = scores

            num_anomalies = sum(result['is_anomaly'])
            logger.info(f"üîç Wykryto {num_anomalies} anomalii w {len(data)} pr√≥bkach")

            return result
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas detekcji anomalii: {e}")
            return pd.DataFrame()

    def _extract_features(self, data: pd.DataFrame) -> np.ndarray:
        """
        Wyodrƒôbnia cechy z danych surowych.

        Args:
            data: DataFrame z danymi surowymi

        Returns:
            Tablica cech do analizy
        """
        # Przekszta≈Çcamy dane do formatu odpowiedniego dla algorytmu
        # Przyk≈Çadowe cechy: zwroty, zmienno≈õƒá, objƒôto≈õƒá
        features = []

        # Podstawowe funkcje
        if 'close' in data.columns:
            # Zwroty procentowe
            returns = data['close'].pct_change().fillna(0).values.reshape(-1, 1)
            features.append(returns)

            # Zmienno≈õƒá (rolling)
            volatility = data['close'].pct_change().rolling(window=20).std().fillna(0).values.reshape(-1, 1)
            features.append(volatility)

        # Cechy wolumenowe
        if 'volume' in data.columns:
            # Wolumen znormalizowany
            volume = data['volume'].values.reshape(-1, 1)
            volume_ma = data['volume'].rolling(window=20).mean().fillna(data['volume'].mean()).values.reshape(-1, 1)
            volume_ratio = (volume / volume_ma)
            features.append(volume_ratio)

        # Spready, je≈õli dostƒôpne
        if all(col in data.columns for col in ['high', 'low']):
            spread = (data['high'] - data['low']).values.reshape(-1, 1)
            features.append(spread)

        # ≈ÅƒÖczymy wszystkie cechy
        if features:
            return np.hstack(features)
        else:
            # Je≈õli nie ma odpowiednich kolumn, u≈ºywamy wszystkich dostƒôpnych danych numerycznych
            return data.select_dtypes(include=[np.number]).values

# Przyk≈Çad u≈ºycia (dla test√≥w)
if __name__ == "__main__":
    # Tworzymy przyk≈Çadowe dane
    np.random.seed(42)
    dates = pd.date_range(start='2023-01-01', periods=1000, freq='H')
    prices = np.cumsum(np.random.normal(0, 1, 1000)) + 1000

    # Dodajemy kilka anomalii
    prices[500:510] += 50
    prices[700:705] -= 30
    prices[900] += 100

    volumes = np.random.normal(1000, 100, 1000)
    volumes[500:510] *= 5

    data = pd.DataFrame({
        'timestamp': dates,
        'close': prices,
        'volume': volumes,
        'high': prices + np.random.normal(0, 5, 1000),
        'low': prices - np.random.normal(0, 5, 1000)
    })

    # Inicjalizujemy i trenujemy detektor
    detector = AnomalyDetector(sensitivity=0.03)
    train_data = data.iloc[:800]
    test_data = data.iloc[800:]

    detector.train(train_data)
    results = detector.detect(test_data)

    if not results.empty:
        print(f"Wykryto {results['is_anomaly'].sum()} anomalii w {len(results)} pr√≥bkach")
        anomalies = results[results['is_anomaly'] == 1]
        if not anomalies.empty:
            print("\nPrzyklady wykrytych anomalii:")
            print(anomalies[['timestamp', 'close', 'volume', 'anomaly_score']].head())