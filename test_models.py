
#!/usr/bin/env python3
"""
test_models.py
-------------
Skrypt do testowania modeli AI w projekcie.
Skanuje folder ai_models/ i testuje modele.
"""

import os
import sys
import argparse
from python_libs.model_tester import ModelTester

def parse_arguments():
    """Parsowanie argument√≥w wiersza polece≈Ñ."""
    parser = argparse.ArgumentParser(description='Testowanie modeli AI')
    parser.add_argument(
        '--path', 
        type=str, 
        default='ai_models',
        help='≈öcie≈ºka do folderu z modelami (domy≈õlnie: ai_models)'
    )
    parser.add_argument(
        '--log', 
        type=str, 
        default='logs/model_check.log',
        help='≈öcie≈ºka do pliku log√≥w (domy≈õlnie: logs/model_check.log)'
    )
    return parser.parse_args()

def main():
    """G≈Ç√≥wna funkcja skryptu."""
    args = parse_arguments()
    
    # Tworzenie katalogu log√≥w, je≈õli nie istnieje
    os.makedirs(os.path.dirname(args.log), exist_ok=True)
    
    print(f"üîç Rozpoczynam testowanie modeli w {args.path}...")
    
    # Uruchomienie testera
    tester = ModelTester(models_path=args.path, log_path=args.log)
    stats = tester.run_tests()
    
    # Wy≈õwietlenie podsumowania
    print("\nüìä Podsumowanie test√≥w:")
    print(f"- Przeskanowano plik√≥w .py: {stats['py_files_scanned']}")
    print(f"- Przeskanowano plik√≥w .pkl: {stats['pkl_files_scanned']}")
    print(f"- Logi zapisano do: {args.log}")
    
    print("\n‚úÖ Testowanie zako≈Ñczone.")

if __name__ == "__main__":
    main()
#!/usr/bin/env python
"""
Test Models

Skrypt sprawdzajƒÖcy, czy wszystkie modele AI sƒÖ prawid≈Çowo ≈Çadowane.
"""

import os
import sys
import logging
import importlib
from pathlib import Path

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("test_models")

def scan_directory(directory="ai_models"):
    """Skanuje wskazany katalog i zwraca listƒô plik√≥w .py"""
    module_files = []
    try:
        for file in Path(directory).glob("*.py"):
            if file.name != "__init__.py" and not file.name.startswith("_"):
                module_files.append(file.stem)
        logger.info(f"Znaleziono {len(module_files)} plik√≥w .py w katalogu {directory}")
        return module_files
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas skanowania katalogu {directory}: {e}")
        return []

def test_model_loader():
    """Testuje modu≈Ç ≈ÇadujƒÖcy modele AI"""
    try:
        # Sprawd≈∫, czy istnieje modu≈Ç model_loader
        from ai_models.model_loader import model_loader
        
        # Sprawd≈∫, czy metoda load_models istnieje
        if hasattr(model_loader, "load_models"):
            model_loader.load_models()
            models = model_loader.get_models_summary()
            logger.info(f"Model loader dzia≈Ça poprawnie. Za≈Çadowano {len(models)} modeli")
            for model in models:
                logger.info(f"  - {model.get('name', 'Nieznany')} ({model.get('type', 'Nieznany typ')})")
            return True
        else:
            logger.error("Model loader nie posiada metody load_models")
            return False
    except ImportError as e:
        logger.error(f"Nie mo≈ºna zaimportowaƒá model_loader: {e}")
        return False
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas testowania model_loader: {e}")
        return False

def test_import_modules(modules):
    """Pr√≥buje zaimportowaƒá wszystkie modu≈Çy z listy"""
    successful = 0
    failed = 0
    
    for module_name in modules:
        try:
            module_path = f"ai_models.{module_name}"
            module = importlib.import_module(module_path)
            logger.info(f"‚úÖ Zaimportowano modu≈Ç: {module_path}")
            
            # Sprawd≈∫ klasy w module
            classes = [name for name, obj in module.__dict__.items() 
                      if isinstance(obj, type) and not name.startswith("_")]
            
            if classes:
                logger.info(f"   Znalezione klasy: {', '.join(classes)}")
            successful += 1
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd importu modu≈Çu {module_name}: {e}")
            failed += 1
    
    return successful, failed

if __name__ == "__main__":
    print("\n===== TEST MODELI AI =====\n")
    
    # Sprawd≈∫, czy katalog ai_models istnieje
    if not os.path.isdir("ai_models"):
        logger.error("Katalog ai_models nie istnieje!")
        sys.exit(1)
    
    # Skanuj katalog ai_models
    modules = scan_directory("ai_models")
    
    # Testuj model loader
    loader_ok = test_model_loader()
    
    # Testuj import modu≈Ç√≥w
    successful, failed = test_import_modules(modules)
    
    # Podsumowanie
    print("\n===== PODSUMOWANIE =====")
    print(f"Znaleziono {len(modules)} modu≈Ç√≥w w katalogu ai_models")
    print(f"Pomy≈õlnie zaimportowano: {successful}")
    print(f"B≈Çƒôdy importu: {failed}")
    print(f"Model loader: {'OK' if loader_ok else 'B≈ÅƒÑD'}")
    
    if failed > 0 or not loader_ok:
        print("\n‚ö†Ô∏è Wykryto problemy z modelami AI!")
        sys.exit(1)
    else:
        print("\n‚úÖ Wszystkie modele AI sƒÖ prawid≈Çowo ≈Çadowane!")
#!/usr/bin/env python3
"""
test_models.py - Skrypt do testowania modeli AI.

Ten skrypt testuje wszystkie modele AI w projekcie, sprawdzajƒÖc ich
poprawno≈õƒá i dzia≈Çanie.
"""

import os
import sys
import logging
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from typing import Dict, List, Any

# Dodaj katalog g≈Ç√≥wny do ≈õcie≈ºki Pythona
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Importuj tester modeli
try:
    from python_libs.model_tester import ModelTester
except ImportError:
    print("‚ùå Nie znaleziono modu≈Çu model_tester w python_libs")
    print("Uruchom najpierw setup_local_packages.py aby zainstalowaƒá wymagane pakiety")
    sys.exit(1)

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("test_models.log"),
        logging.StreamHandler()
    ]
)

def generate_test_data() -> tuple:
    """
    Generuje dane do testowania modeli.
    
    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    # Generuj dane do klasyfikacji
    X, y = make_classification(
        n_samples=1000, 
        n_features=20, 
        n_informative=10, 
        n_classes=2, 
        random_state=42
    )
    
    # Podziel na zestaw treningowy i testowy
    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    return X_train, X_test, y_train, y_test

def test_models() -> Dict[str, Any]:
    """
    Testuje wszystkie modele w projekcie.
    
    Returns:
        Dict[str, Any]: Wyniki test√≥w
    """
    print("üîç Rozpoczƒôcie testowania modeli AI...")
    
    # Inicjalizuj tester modeli
    model_tester = ModelTester(models_path='ai_models', log_path='logs/model_tests.log')
    
    # Uruchom testy
    results = model_tester.run_tests()
    
    # Pobierz za≈Çadowane modele
    models = model_tester.get_loaded_models()
    
    # Wygeneruj dane testowe
    X_train, X_test, y_train, y_test = generate_test_data()
    
    # Testuj modele typu ML z metodami fit/predict
    ml_results = {}
    for model_info in models:
        model_name = model_info['name']
        instance = model_info['instance']
        
        # Sprawd≈∫ czy model ma metody fit i predict
        if model_info['has_fit'] and model_info['has_predict']:
            print(f"‚è≥ Testowanie modelu ML: {model_name}...")
            
            try:
                # Trenuj model
                instance.fit(X_train, y_train)
                
                # Ocena modelu
                evaluation = model_tester.evaluate_model(model_name, X_test, y_test)
                ml_results[model_name] = evaluation
                
                # Zapisz accuracy w informacjach o modelu
                if 'accuracy' in evaluation:
                    print(f"‚úÖ Model {model_name}: accuracy = {evaluation['accuracy']:.4f}")
                else:
                    print(f"‚ö†Ô∏è Model {model_name}: brak metryki accuracy")
            except Exception as e:
                print(f"‚ùå B≈ÇƒÖd podczas testowania modelu {model_name}: {e}")
                ml_results[model_name] = {"error": str(e)}
    
    # Zapisz metadane modeli
    model_tester.save_model_metadata("model_metadata.json")
    
    print("\nüìã Podsumowanie test√≥w modeli:")
    print(f"- Wykryto {results.get('models_detected', 0)} modeli")
    print(f"- Za≈Çadowano {results.get('models_loaded', 0)} modeli")
    print(f"- Przetestowano {len(ml_results)} modeli ML")
    
    if results.get('errors', []):
        print("\n‚ö†Ô∏è Problemy podczas test√≥w:")
        for error in results.get('errors', []):
            print(f"- {error}")
            
    return {
        "results": results,
        "ml_results": ml_results,
        "models": [model['name'] for model in models]
    }

def main():
    """
    Funkcja g≈Ç√≥wna.
    """
    try:
        results = test_models()
        
        print("\n‚úÖ Testowanie modeli zako≈Ñczone pomy≈õlnie!")
        return 0
    except Exception as e:
        print(f"\n‚ùå B≈ÇƒÖd podczas testowania modeli: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
