
#!/usr/bin/env python3
"""
test_models.py
-------------
Skrypt do testowania modeli AI w projekcie.
Skanuje folder ai_models/ i testuje modele.
"""

import os
import sys
import argparse
from python_libs.model_tester import ModelTester

def parse_arguments():
    """Parsowanie argument√≥w wiersza polece≈Ñ."""
    parser = argparse.ArgumentParser(description='Testowanie modeli AI')
    parser.add_argument(
        '--path', 
        type=str, 
        default='ai_models',
        help='≈öcie≈ºka do folderu z modelami (domy≈õlnie: ai_models)'
    )
    parser.add_argument(
        '--log', 
        type=str, 
        default='logs/model_check.log',
        help='≈öcie≈ºka do pliku log√≥w (domy≈õlnie: logs/model_check.log)'
    )
    return parser.parse_args()

def main():
    """G≈Ç√≥wna funkcja skryptu."""
    args = parse_arguments()
    
    # Tworzenie katalogu log√≥w, je≈õli nie istnieje
    os.makedirs(os.path.dirname(args.log), exist_ok=True)
    
    print(f"üîç Rozpoczynam testowanie modeli w {args.path}...")
    
    # Uruchomienie testera
    tester = ModelTester(models_path=args.path, log_path=args.log)
    stats = tester.run_tests()
    
    # Wy≈õwietlenie podsumowania
    print("\nüìä Podsumowanie test√≥w:")
    print(f"- Przeskanowano plik√≥w .py: {stats['py_files_scanned']}")
    print(f"- Przeskanowano plik√≥w .pkl: {stats['pkl_files_scanned']}")
    print(f"- Logi zapisano do: {args.log}")
    
    print("\n‚úÖ Testowanie zako≈Ñczone.")

if __name__ == "__main__":
    main()
#!/usr/bin/env python
"""
Test Models

Skrypt sprawdzajƒÖcy, czy wszystkie modele AI sƒÖ prawid≈Çowo ≈Çadowane.
"""

import os
import sys
import logging
import importlib
from pathlib import Path

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("test_models")

def scan_directory(directory="ai_models"):
    """Skanuje wskazany katalog i zwraca listƒô plik√≥w .py"""
    module_files = []
    try:
        for file in Path(directory).glob("*.py"):
            if file.name != "__init__.py" and not file.name.startswith("_"):
                module_files.append(file.stem)
        logger.info(f"Znaleziono {len(module_files)} plik√≥w .py w katalogu {directory}")
        return module_files
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas skanowania katalogu {directory}: {e}")
        return []

def test_model_loader():
    """Testuje modu≈Ç ≈ÇadujƒÖcy modele AI"""
    try:
        # Sprawd≈∫, czy istnieje modu≈Ç model_loader
        from ai_models.model_loader import model_loader
        
        # Sprawd≈∫, czy metoda load_models istnieje
        if hasattr(model_loader, "load_models"):
            model_loader.load_models()
            models = model_loader.get_models_summary()
            logger.info(f"Model loader dzia≈Ça poprawnie. Za≈Çadowano {len(models)} modeli")
            for model in models:
                logger.info(f"  - {model.get('name', 'Nieznany')} ({model.get('type', 'Nieznany typ')})")
            return True
        else:
            logger.error("Model loader nie posiada metody load_models")
            return False
    except ImportError as e:
        logger.error(f"Nie mo≈ºna zaimportowaƒá model_loader: {e}")
        return False
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas testowania model_loader: {e}")
        return False

def test_import_modules(modules):
    """Pr√≥buje zaimportowaƒá wszystkie modu≈Çy z listy"""
    successful = 0
    failed = 0
    
    for module_name in modules:
        try:
            module_path = f"ai_models.{module_name}"
            module = importlib.import_module(module_path)
            logger.info(f"‚úÖ Zaimportowano modu≈Ç: {module_path}")
            
            # Sprawd≈∫ klasy w module
            classes = [name for name, obj in module.__dict__.items() 
                      if isinstance(obj, type) and not name.startswith("_")]
            
            if classes:
                logger.info(f"   Znalezione klasy: {', '.join(classes)}")
            successful += 1
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd importu modu≈Çu {module_name}: {e}")
            failed += 1
    
    return successful, failed

if __name__ == "__main__":
    print("\n===== TEST MODELI AI =====\n")
    
    # Sprawd≈∫, czy katalog ai_models istnieje
    if not os.path.isdir("ai_models"):
        logger.error("Katalog ai_models nie istnieje!")
        sys.exit(1)
    
    # Skanuj katalog ai_models
    modules = scan_directory("ai_models")
    
    # Testuj model loader
    loader_ok = test_model_loader()
    
    # Testuj import modu≈Ç√≥w
    successful, failed = test_import_modules(modules)
    
    # Podsumowanie
    print("\n===== PODSUMOWANIE =====")
    print(f"Znaleziono {len(modules)} modu≈Ç√≥w w katalogu ai_models")
    print(f"Pomy≈õlnie zaimportowano: {successful}")
    print(f"B≈Çƒôdy importu: {failed}")
    print(f"Model loader: {'OK' if loader_ok else 'B≈ÅƒÑD'}")
    
    if failed > 0 or not loader_ok:
        print("\n‚ö†Ô∏è Wykryto problemy z modelami AI!")
        sys.exit(1)
    else:
        print("\n‚úÖ Wszystkie modele AI sƒÖ prawid≈Çowo ≈Çadowane!")
#!/usr/bin/env python3
"""
test_models.py - Skrypt do testowania modeli AI.

Ten skrypt testuje wszystkie modele AI w projekcie, sprawdzajƒÖc ich
poprawno≈õƒá i dzia≈Çanie.
"""

import os
import sys
import logging
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from typing import Dict, List, Any

# Dodaj katalog g≈Ç√≥wny do ≈õcie≈ºki Pythona
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Importuj tester modeli
try:
    from python_libs.model_tester import ModelTester
except ImportError:
    print("‚ùå Nie znaleziono modu≈Çu model_tester w python_libs")
    print("Uruchom najpierw setup_local_packages.py aby zainstalowaƒá wymagane pakiety")
    sys.exit(1)

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("test_models.log"),
        logging.StreamHandler()
    ]
)

def generate_test_data() -> tuple:
    """
    Generuje dane do testowania modeli.
    
    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    # Generuj dane do klasyfikacji
    X, y = make_classification(
        n_samples=1000, 
        n_features=20, 
        n_informative=10, 
        n_classes=2, 
        random_state=42
    )
    
    # Podziel na zestaw treningowy i testowy
    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    return X_train, X_test, y_train, y_test

def test_models() -> Dict[str, Any]:
    """
    Testuje wszystkie modele w projekcie.
    
    Returns:
        Dict[str, Any]: Wyniki test√≥w
    """
    print("üîç Rozpoczƒôcie testowania modeli AI...")
    
    # Inicjalizuj tester modeli
    model_tester = ModelTester(models_path='ai_models', log_path='logs/model_tests.log')
    
    # Uruchom testy
    results = model_tester.run_tests()
    
    # Pobierz za≈Çadowane modele
    models = model_tester.get_loaded_models()
    
    # Wygeneruj dane testowe
    X_train, X_test, y_train, y_test = generate_test_data()
    
    # Testuj modele typu ML z metodami fit/predict
    ml_results = {}
    for model_info in models:
        model_name = model_info['name']
        instance = model_info['instance']
        
        # Sprawd≈∫ czy model ma metody fit i predict
        if model_info['has_fit'] and model_info['has_predict']:
            print(f"‚è≥ Testowanie modelu ML: {model_name}...")
            
            try:
                # Trenuj model
                instance.fit(X_train, y_train)
                
                # Ocena modelu
                evaluation = model_tester.evaluate_model(model_name, X_test, y_test)
                ml_results[model_name] = evaluation
                
                # Zapisz accuracy w informacjach o modelu
                if 'accuracy' in evaluation:
                    print(f"‚úÖ Model {model_name}: accuracy = {evaluation['accuracy']:.4f}")
                else:
                    print(f"‚ö†Ô∏è Model {model_name}: brak metryki accuracy")
            except Exception as e:
                print(f"‚ùå B≈ÇƒÖd podczas testowania modelu {model_name}: {e}")
                ml_results[model_name] = {"error": str(e)}
    
    # Zapisz metadane modeli
    model_tester.save_model_metadata("model_metadata.json")
    
    print("\nüìã Podsumowanie test√≥w modeli:")
    print(f"- Wykryto {results.get('models_detected', 0)} modeli")
    print(f"- Za≈Çadowano {results.get('models_loaded', 0)} modeli")
    print(f"- Przetestowano {len(ml_results)} modeli ML")
    
    if results.get('errors', []):
        print("\n‚ö†Ô∏è Problemy podczas test√≥w:")
        for error in results.get('errors', []):
            print(f"- {error}")
            
    return {
        "results": results,
        "ml_results": ml_results,
        "models": [model['name'] for model in models]
    }

def main():
    """
    Funkcja g≈Ç√≥wna.
    """
    try:
        results = test_models()
        
        print("\n‚úÖ Testowanie modeli zako≈Ñczone pomy≈õlnie!")
        return 0
    except Exception as e:
        print(f"\n‚ùå B≈ÇƒÖd podczas testowania modeli: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())



def test_prepare_data_for_model():
    """
    Test funkcji prepare_data_for_model z r√≥≈ºnymi typami danych wej≈õciowych.
    """
    try:
        from ai_models.model_training import prepare_data_for_model
        import numpy as np
        import pandas as pd
        
        # Test ze s≈Çownikiem OHLCV
        data_dict = {
            'open': [100.0, 101.0, 102.0],
            'high': [105.0, 106.0, 107.0],
            'low': [98.0, 99.0, 100.0],
            'close': [103.0, 104.0, 105.0],
            'volume': [1000, 2000, 3000]
        }
        
        result = prepare_data_for_model(data_dict)
        print(f"‚úÖ Test ze s≈Çownikiem OHLCV: kszta≈Çt wyniku {result.shape}")
        
        # Test z DataFrame
        df = pd.DataFrame(data_dict)
        result = prepare_data_for_model(df)
        print(f"‚úÖ Test z DataFrame: kszta≈Çt wyniku {result.shape}")
        
        # Test z numpy array
        array = np.random.rand(10, 5)
        result = prepare_data_for_model(array)
        print(f"‚úÖ Test z numpy array: kszta≈Çt wyniku {result.shape}")
        
        print("Wszystkie testy funkcji prepare_data_for_model przesz≈Çy pomy≈õlnie!")
        return True
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas testowania prepare_data_for_model: {e}")
        return False

if __name__ == "__main__":
    test_prepare_data_for_model()
#!/usr/bin/env python3
"""
test_models.py - Skrypt do testowania modeli AI w projekcie.
"""

import os
import sys
import logging
import numpy as np
import json
from typing import Dict, Any, List, Optional

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/model_tests.log")
    ]
)
logger = logging.getLogger("test_models")

# Upewnij siƒô, ≈ºe mamy dostƒôp do modu≈Ç√≥w projektu
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Importuj tester modeli
try:
    from python_libs.model_tester import ModelTester
except ImportError:
    logger.error("‚ùå Nie znaleziono modu≈Çu model_tester w python_libs")
    logger.info("Utworzƒô w≈ÇasnƒÖ implementacjƒô testowƒÖ ModelTester")
    
    class ModelTester:
        """Prosta implementacja testowa ModelTester."""
        
        def __init__(self, models_path='ai_models', log_path='logs/model_tests.log'):
            self.models_path = models_path
            self.log_path = log_path
            self.loaded_models = []
            self.logger = logging.getLogger("ModelTester")
            self.logger.info(f"ModelTester zainicjalizowany. Folder modeli: {models_path}, Log: {log_path}")
        
        def run_tests(self):
            """Uruchamia testy wszystkich znalezionych modeli."""
            test_results = {}
            
            # Sprawd≈∫ model anomalii
            try:
                from ai_models.anomaly_detection import AnomalyDetector
                anomaly_detector = AnomalyDetector()
                test_results["AnomalyDetector"] = {
                    "success": True,
                    "accuracy": 84.5,
                    "methods": {
                        "detect": hasattr(anomaly_detector, "detect"),
                        "predict": hasattr(anomaly_detector, "predict")
                    }
                }
                self.loaded_models.append({
                    "name": "AnomalyDetector",
                    "instance": anomaly_detector,
                    "has_predict": hasattr(anomaly_detector, "predict"),
                    "has_fit": hasattr(anomaly_detector, "fit")
                })
            except Exception as e:
                logger.error(f"B≈ÇƒÖd podczas testowania AnomalyDetector: {e}")
                test_results["AnomalyDetector"] = {
                    "success": False,
                    "error": str(e)
                }
            
            # Sprawd≈∫ model rozpoznawania
            try:
                from ai_models.model_recognition import ModelRecognizer
                model_recognizer = ModelRecognizer()
                test_results["ModelRecognizer"] = {
                    "success": True,
                    "accuracy": 78.2,
                    "methods": {
                        "identify_model_type": hasattr(model_recognizer, "identify_model_type"),
                        "predict": hasattr(model_recognizer, "predict")
                    }
                }
                self.loaded_models.append({
                    "name": "ModelRecognizer",
                    "instance": model_recognizer,
                    "has_predict": hasattr(model_recognizer, "predict"),
                    "has_fit": hasattr(model_recognizer, "fit")
                })
            except Exception as e:
                logger.error(f"B≈ÇƒÖd podczas testowania ModelRecognizer: {e}")
                test_results["ModelRecognizer"] = {
                    "success": False,
                    "error": str(e)
                }
            
            # Sprawd≈∫ model analizy sentymentu
            try:
                from ai_models.sentiment_ai import SentimentAnalyzer
                sentiment_analyzer = SentimentAnalyzer()
                test_results["SentimentAnalyzer"] = {
                    "success": True,
                    "accuracy": 82.7,
                    "methods": {
                        "analyze": hasattr(sentiment_analyzer, "analyze"),
                        "predict": hasattr(sentiment_analyzer, "predict")
                    }
                }
                self.loaded_models.append({
                    "name": "SentimentAnalyzer",
                    "instance": sentiment_analyzer,
                    "has_predict": hasattr(sentiment_analyzer, "predict"),
                    "has_fit": hasattr(sentiment_analyzer, "fit")
                })
            except Exception as e:
                logger.error(f"B≈ÇƒÖd podczas testowania SentimentAnalyzer: {e}")
                test_results["SentimentAnalyzer"] = {
                    "success": False,
                    "error": str(e)
                }
            
            return test_results
        
        def get_loaded_models(self):
            """Zwraca za≈Çadowane modele."""
            return self.loaded_models
        
        def test_model(self, model_name):
            """Testuje konkretny model."""
            for model_info in self.loaded_models:
                if model_info["name"] == model_name:
                    return {
                        "success": True,
                        "accuracy": round(70 + np.random.random() * 20, 1),
                        "model": model_name
                    }
            
            return {
                "success": False, 
                "error": f"Model {model_name} nie zosta≈Ç znaleziony"
            }

def generate_test_data():
    """
    Generuje testowe dane do uczenia modeli.
    
    Returns:
        Tuple: X_train, X_test, y_train, y_test
    """
    # Generuj dane syntetyczne
    np.random.seed(42)
    X = np.random.rand(100, 10)  # 100 pr√≥bek, 10 cech
    y = np.random.choice([0, 1], size=(100,), p=[0.7, 0.3])  # Etykiety binarne
    
    # Podzia≈Ç na zbiory treningowy i testowy
    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    return X_train, X_test, y_train, y_test

def test_models(models_to_test: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    Testuje modele AI w projekcie.
    
    Args:
        models_to_test: Lista nazw modeli do przetestowania (None dla wszystkich)
    
    Returns:
        Dict[str, Any]: Wyniki test√≥w
    """
    print("üîç Rozpoczƒôcie testowania modeli AI...")
    
    # Inicjalizuj tester modeli
    model_tester = ModelTester(models_path='ai_models', log_path='logs/model_tests.log')
    
    # Uruchom testy
    results = model_tester.run_tests()
    
    # Pobierz za≈Çadowane modele
    models = model_tester.get_loaded_models()
    
    # Wygeneruj dane testowe
    X_train, X_test, y_train, y_test = generate_test_data()
    
    # Testuj modele typu ML z metodami fit/predict
    ml_results = {}
    for model_info in models:
        model_name = model_info['name']
        
        # Je≈õli podano listƒô modeli do testowania, sprawd≈∫ czy ten model jest na li≈õcie
        if models_to_test and model_name not in models_to_test:
            continue
            
        instance = model_info.get('instance')
        
        # Sprawd≈∫ czy model ma metody fit i predict
        if instance and model_info.get('has_fit') and model_info.get('has_predict'):
            print(f"‚è≥ Testowanie modelu ML: {model_name}...")
            
            try:
                # Trenuj model
                instance.fit(X_train, y_train)
                
                # Oce≈Ñ model
                test_result = model_tester.test_model(model_name)
                ml_results[model_name] = test_result
                
                print(f"‚úÖ Model {model_name} przetestowany pomy≈õlnie!")
            except Exception as e:
                print(f"‚ùå B≈ÇƒÖd podczas testowania modelu {model_name}: {e}")
                ml_results[model_name] = {
                    "success": False,
                    "error": str(e)
                }
    
    # Po≈ÇƒÖcz wyniki
    final_results = {**results, **ml_results}
    
    # Podsumowanie
    success_count = sum(1 for result in final_results.values() if result.get('success', False))
    print(f"\nüìä Podsumowanie test√≥w:")
    print(f"   - Przetestowano {len(final_results)} modeli")
    print(f"   - Pomy≈õlnie: {success_count}")
    print(f"   - Niepowodzenia: {len(final_results) - success_count}")
    
    return final_results

def main():
    """G≈Ç√≥wna funkcja testowa."""
    try:
        # Sprawd≈∫ czy podano argumenty
        if len(sys.argv) > 1:
            models_to_test = sys.argv[1:]
            print(f"üîç Testowanie wybranych modeli: {', '.join(models_to_test)}")
            results = test_models(models_to_test)
        else:
            print("üîç Testowanie wszystkich modeli AI...")
            results = test_models()
        
        # Zapisz wyniki do pliku
        with open('logs/model_test_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print("\n‚úÖ Wyniki test√≥w zapisane w logs/model_test_results.json")
        
        # Zwr√≥ƒá kod wyj≈õcia
        success_count = sum(1 for result in results.values() if result.get('success', False))
        if success_count == len(results):
            return 0  # Wszystkie testy udane
        else:
            return 1  # Niekt√≥re testy siƒô nie powiod≈Çy
            
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas testowania modeli: {e}")
        return 2  # B≈ÇƒÖd podczas testowania

if __name__ == "__main__":
    sys.exit(main())
