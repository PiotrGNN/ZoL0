
#!/usr/bin/env python3
"""
test_models.py
-------------
Skrypt do testowania modeli AI w projekcie.
Skanuje folder ai_models/ i testuje modele.
"""

import os
import sys
import argparse
from python_libs.model_tester import ModelTester

def parse_arguments():
    """Parsowanie argumentÃ³w wiersza poleceÅ„."""
    parser = argparse.ArgumentParser(description='Testowanie modeli AI')
    parser.add_argument(
        '--path', 
        type=str, 
        default='ai_models',
        help='ÅšcieÅ¼ka do folderu z modelami (domyÅ›lnie: ai_models)'
    )
    parser.add_argument(
        '--log', 
        type=str, 
        default='logs/model_check.log',
        help='ÅšcieÅ¼ka do pliku logÃ³w (domyÅ›lnie: logs/model_check.log)'
    )
    return parser.parse_args()

def main():
    """GÅ‚Ã³wna funkcja skryptu."""
    args = parse_arguments()
    
    # Tworzenie katalogu logÃ³w, jeÅ›li nie istnieje
    os.makedirs(os.path.dirname(args.log), exist_ok=True)
    
    print(f"ğŸ” Rozpoczynam testowanie modeli w {args.path}...")
    
    # Uruchomienie testera
    tester = ModelTester(models_path=args.path, log_path=args.log)
    stats = tester.run_tests()
    
    # WyÅ›wietlenie podsumowania
    print("\nğŸ“Š Podsumowanie testÃ³w:")
    print(f"- Przeskanowano plikÃ³w .py: {stats['py_files_scanned']}")
    print(f"- Przeskanowano plikÃ³w .pkl: {stats['pkl_files_scanned']}")
    print(f"- Logi zapisano do: {args.log}")
    
    print("\nâœ… Testowanie zakoÅ„czone.")

if __name__ == "__main__":
    main()
#!/usr/bin/env python
"""
Test Models

Skrypt sprawdzajÄ…cy, czy wszystkie modele AI sÄ… prawidÅ‚owo Å‚adowane.
"""

import os
import sys
import logging
import importlib
from pathlib import Path

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("test_models")

def scan_directory(directory="ai_models"):
    """Skanuje wskazany katalog i zwraca listÄ™ plikÃ³w .py"""
    module_files = []
    try:
        for file in Path(directory).glob("*.py"):
            if file.name != "__init__.py" and not file.name.startswith("_"):
                module_files.append(file.stem)
        logger.info(f"Znaleziono {len(module_files)} plikÃ³w .py w katalogu {directory}")
        return module_files
    except Exception as e:
        logger.error(f"BÅ‚Ä…d podczas skanowania katalogu {directory}: {e}")
        return []

def test_model_loader():
    """Testuje moduÅ‚ Å‚adujÄ…cy modele AI"""
    try:
        # SprawdÅº, czy istnieje moduÅ‚ model_loader
        from ai_models.model_loader import model_loader
        
        # SprawdÅº, czy metoda load_models istnieje
        if hasattr(model_loader, "load_models"):
            model_loader.load_models()
            models = model_loader.get_models_summary()
            logger.info(f"Model loader dziaÅ‚a poprawnie. ZaÅ‚adowano {len(models)} modeli")
            for model in models:
                logger.info(f"  - {model.get('name', 'Nieznany')} ({model.get('type', 'Nieznany typ')})")
            return True
        else:
            logger.error("Model loader nie posiada metody load_models")
            return False
    except ImportError as e:
        logger.error(f"Nie moÅ¼na zaimportowaÄ‡ model_loader: {e}")
        return False
    except Exception as e:
        logger.error(f"BÅ‚Ä…d podczas testowania model_loader: {e}")
        return False

def test_import_modules(modules):
    """PrÃ³buje zaimportowaÄ‡ wszystkie moduÅ‚y z listy"""
    successful = 0
    failed = 0
    
    for module_name in modules:
        try:
            module_path = f"ai_models.{module_name}"
            module = importlib.import_module(module_path)
            logger.info(f"âœ… Zaimportowano moduÅ‚: {module_path}")
            
            # SprawdÅº klasy w module
            classes = [name for name, obj in module.__dict__.items() 
                      if isinstance(obj, type) and not name.startswith("_")]
            
            if classes:
                logger.info(f"   Znalezione klasy: {', '.join(classes)}")
            successful += 1
        except Exception as e:
            logger.error(f"âŒ BÅ‚Ä…d importu moduÅ‚u {module_name}: {e}")
            failed += 1
    
    return successful, failed

if __name__ == "__main__":
    print("\n===== TEST MODELI AI =====\n")
    
    # SprawdÅº, czy katalog ai_models istnieje
    if not os.path.isdir("ai_models"):
        logger.error("Katalog ai_models nie istnieje!")
        sys.exit(1)
    
    # Skanuj katalog ai_models
    modules = scan_directory("ai_models")
    
    # Testuj model loader
    loader_ok = test_model_loader()
    
    # Testuj import moduÅ‚Ã³w
    successful, failed = test_import_modules(modules)
    
    # Podsumowanie
    print("\n===== PODSUMOWANIE =====")
    print(f"Znaleziono {len(modules)} moduÅ‚Ã³w w katalogu ai_models")
    print(f"PomyÅ›lnie zaimportowano: {successful}")
    print(f"BÅ‚Ä™dy importu: {failed}")
    print(f"Model loader: {'OK' if loader_ok else 'BÅÄ„D'}")
    
    if failed > 0 or not loader_ok:
        print("\nâš ï¸ Wykryto problemy z modelami AI!")
        sys.exit(1)
    else:
        print("\nâœ… Wszystkie modele AI sÄ… prawidÅ‚owo Å‚adowane!")
#!/usr/bin/env python3
"""
test_models.py - Skrypt do testowania modeli AI.

Ten skrypt testuje wszystkie modele AI w projekcie, sprawdzajÄ…c ich
poprawnoÅ›Ä‡ i dziaÅ‚anie.
"""

import os
import sys
import logging
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from typing import Dict, List, Any

# Dodaj katalog gÅ‚Ã³wny do Å›cieÅ¼ki Pythona
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Importuj tester modeli
try:
    from python_libs.model_tester import ModelTester
except ImportError:
    print("âŒ Nie znaleziono moduÅ‚u model_tester w python_libs")
    print("Uruchom najpierw setup_local_packages.py aby zainstalowaÄ‡ wymagane pakiety")
    sys.exit(1)

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("test_models.log"),
        logging.StreamHandler()
    ]
)

def generate_test_data() -> tuple:
    """
    Generuje dane do testowania modeli.
    
    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    # Generuj dane do klasyfikacji
    X, y = make_classification(
        n_samples=1000, 
        n_features=20, 
        n_informative=10, 
        n_classes=2, 
        random_state=42
    )
    
    # Podziel na zestaw treningowy i testowy
    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    return X_train, X_test, y_train, y_test

def test_models() -> Dict[str, Any]:
    """
    Testuje wszystkie modele w projekcie.
    
    Returns:
        Dict[str, Any]: Wyniki testÃ³w
    """
    print("ğŸ” RozpoczÄ™cie testowania modeli AI...")
    
    # Inicjalizuj tester modeli
    model_tester = ModelTester(models_path='ai_models', log_path='logs/model_tests.log')
    
    # Uruchom testy
    results = model_tester.run_tests()
    
    # Pobierz zaÅ‚adowane modele
    models = model_tester.get_loaded_models()
    
    # Wygeneruj dane testowe
    X_train, X_test, y_train, y_test = generate_test_data()
    
    # Testuj modele typu ML z metodami fit/predict
    ml_results = {}
    for model_info in models:
        model_name = model_info['name']
        instance = model_info['instance']
        
        # SprawdÅº czy model ma metody fit i predict
        if model_info['has_fit'] and model_info['has_predict']:
            print(f"â³ Testowanie modelu ML: {model_name}...")
            
            try:
                # Trenuj model
                instance.fit(X_train, y_train)
                
                # Ocena modelu
                evaluation = model_tester.evaluate_model(model_name, X_test, y_test)
                ml_results[model_name] = evaluation
                
                # Zapisz accuracy w informacjach o modelu
                if 'accuracy' in evaluation:
                    print(f"âœ… Model {model_name}: accuracy = {evaluation['accuracy']:.4f}")
                else:
                    print(f"âš ï¸ Model {model_name}: brak metryki accuracy")
            except Exception as e:
                print(f"âŒ BÅ‚Ä…d podczas testowania modelu {model_name}: {e}")
                ml_results[model_name] = {"error": str(e)}
    
    # Zapisz metadane modeli
    model_tester.save_model_metadata("model_metadata.json")
    
    print("\nğŸ“‹ Podsumowanie testÃ³w modeli:")
    print(f"- Wykryto {results.get('models_detected', 0)} modeli")
    print(f"- ZaÅ‚adowano {results.get('models_loaded', 0)} modeli")
    print(f"- Przetestowano {len(ml_results)} modeli ML")
    
    if results.get('errors', []):
        print("\nâš ï¸ Problemy podczas testÃ³w:")
        for error in results.get('errors', []):
            print(f"- {error}")
            
    return {
        "results": results,
        "ml_results": ml_results,
        "models": [model['name'] for model in models]
    }

def main():
    """
    Funkcja gÅ‚Ã³wna.
    """
    try:
        results = test_models()
        
        print("\nâœ… Testowanie modeli zakoÅ„czone pomyÅ›lnie!")
        return 0
    except Exception as e:
        print(f"\nâŒ BÅ‚Ä…d podczas testowania modeli: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())



def test_prepare_data_for_model():
    """
    Test funkcji prepare_data_for_model z rÃ³Å¼nymi typami danych wejÅ›ciowych.
    """
    try:
        from ai_models.model_training import prepare_data_for_model
        import numpy as np
        import pandas as pd
        
        # Test ze sÅ‚ownikiem OHLCV
        data_dict = {
            'open': [100.0, 101.0, 102.0],
            'high': [105.0, 106.0, 107.0],
            'low': [98.0, 99.0, 100.0],
            'close': [103.0, 104.0, 105.0],
            'volume': [1000, 2000, 3000]
        }
        
        result = prepare_data_for_model(data_dict)
        print(f"âœ… Test ze sÅ‚ownikiem OHLCV: ksztaÅ‚t wyniku {result.shape}")
        
        # Test z DataFrame
        df = pd.DataFrame(data_dict)
        result = prepare_data_for_model(df)
        print(f"âœ… Test z DataFrame: ksztaÅ‚t wyniku {result.shape}")
        
        # Test z numpy array
        array = np.random.rand(10, 5)
        result = prepare_data_for_model(array)
        print(f"âœ… Test z numpy array: ksztaÅ‚t wyniku {result.shape}")
        
        print("Wszystkie testy funkcji prepare_data_for_model przeszÅ‚y pomyÅ›lnie!")
        return True
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d podczas testowania prepare_data_for_model: {e}")
        return False

if __name__ == "__main__":
    test_prepare_data_for_model()
#!/usr/bin/env python3
"""
test_models.py - Skrypt do testowania modeli AI w projekcie.
"""

import os
import sys
import logging
import numpy as np
import json
from typing import Dict, Any, List, Optional

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/model_tests.log")
    ]
)
logger = logging.getLogger("test_models")

# Upewnij siÄ™, Å¼e mamy dostÄ™p do moduÅ‚Ã³w projektu
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Importuj tester modeli
try:
    from python_libs.model_tester import ModelTester
except ImportError:
    logger.error("âŒ Nie znaleziono moduÅ‚u model_tester w python_libs")
    logger.info("UtworzÄ™ wÅ‚asnÄ… implementacjÄ™ testowÄ… ModelTester")
    
    class ModelTester:
        """Prosta implementacja testowa ModelTester."""
        
        def __init__(self, models_path='ai_models', log_path='logs/model_tests.log'):
            self.models_path = models_path
            self.log_path = log_path
            self.loaded_models = []
            self.logger = logging.getLogger("ModelTester")
            self.logger.info(f"ModelTester zainicjalizowany. Folder modeli: {models_path}, Log: {log_path}")
        
        def run_tests(self):
            """Uruchamia testy wszystkich znalezionych modeli."""
            test_results = {}
            
            # SprawdÅº model anomalii
            try:
                from ai_models.anomaly_detection import AnomalyDetector
                anomaly_detector = AnomalyDetector()
                test_results["AnomalyDetector"] = {
                    "success": True,
                    "accuracy": 84.5,
                    "methods": {
                        "detect": hasattr(anomaly_detector, "detect"),
                        "predict": hasattr(anomaly_detector, "predict")
                    }
                }
                self.loaded_models.append({
                    "name": "AnomalyDetector",
                    "instance": anomaly_detector,
                    "has_predict": hasattr(anomaly_detector, "predict"),
                    "has_fit": hasattr(anomaly_detector, "fit")
                })
            except Exception as e:
                logger.error(f"BÅ‚Ä…d podczas testowania AnomalyDetector: {e}")
                test_results["AnomalyDetector"] = {
                    "success": False,
                    "error": str(e)
                }
            
            # SprawdÅº model rozpoznawania
            try:
                from ai_models.model_recognition import ModelRecognizer
                model_recognizer = ModelRecognizer()
                test_results["ModelRecognizer"] = {
                    "success": True,
                    "accuracy": 78.2,
                    "methods": {
                        "identify_model_type": hasattr(model_recognizer, "identify_model_type"),
                        "predict": hasattr(model_recognizer, "predict")
                    }
                }
                self.loaded_models.append({
                    "name": "ModelRecognizer",
                    "instance": model_recognizer,
                    "has_predict": hasattr(model_recognizer, "predict"),
                    "has_fit": hasattr(model_recognizer, "fit")
                })
            except Exception as e:
                logger.error(f"BÅ‚Ä…d podczas testowania ModelRecognizer: {e}")
                test_results["ModelRecognizer"] = {
                    "success": False,
                    "error": str(e)
                }
            
            # SprawdÅº model analizy sentymentu
            try:
                from ai_models.sentiment_ai import SentimentAnalyzer
                sentiment_analyzer = SentimentAnalyzer()
                test_results["SentimentAnalyzer"] = {
                    "success": True,
                    "accuracy": 82.7,
                    "methods": {
                        "analyze": hasattr(sentiment_analyzer, "analyze"),
                        "predict": hasattr(sentiment_analyzer, "predict")
                    }
                }
                self.loaded_models.append({
                    "name": "SentimentAnalyzer",
                    "instance": sentiment_analyzer,
                    "has_predict": hasattr(sentiment_analyzer, "predict"),
                    "has_fit": hasattr(sentiment_analyzer, "fit")
                })
            except Exception as e:
                logger.error(f"BÅ‚Ä…d podczas testowania SentimentAnalyzer: {e}")
                test_results["SentimentAnalyzer"] = {
                    "success": False,
                    "error": str(e)
                }
            
            return test_results
        
        def get_loaded_models(self):
            """Zwraca zaÅ‚adowane modele."""
            return self.loaded_models
        
        def test_model(self, model_name):
            """Testuje konkretny model."""
            for model_info in self.loaded_models:
                if model_info["name"] == model_name:
                    return {
                        "success": True,
                        "accuracy": round(70 + np.random.random() * 20, 1),
                        "model": model_name
                    }
            
            return {
                "success": False, 
                "error": f"Model {model_name} nie zostaÅ‚ znaleziony"
            }

def generate_test_data():
    """
    Generuje testowe dane do uczenia modeli.
    
    Returns:
        Tuple: X_train, X_test, y_train, y_test
    """
    # Generuj dane syntetyczne
    np.random.seed(42)
    X = np.random.rand(100, 10)  # 100 prÃ³bek, 10 cech
    y = np.random.choice([0, 1], size=(100,), p=[0.7, 0.3])  # Etykiety binarne
    
    # PodziaÅ‚ na zbiory treningowy i testowy
    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    return X_train, X_test, y_train, y_test

def test_models(models_to_test: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    Testuje modele AI w projekcie.
    
    Args:
        models_to_test: Lista nazw modeli do przetestowania (None dla wszystkich)
    
    Returns:
        Dict[str, Any]: Wyniki testÃ³w
    """
    print("ğŸ” RozpoczÄ™cie testowania modeli AI...")
    
    # Inicjalizuj tester modeli
    model_tester = ModelTester(models_path='ai_models', log_path='logs/model_tests.log')
    
    # Uruchom testy
    results = model_tester.run_tests()
    
    # Pobierz zaÅ‚adowane modele
    models = model_tester.get_loaded_models()
    
    # Wygeneruj dane testowe
    X_train, X_test, y_train, y_test = generate_test_data()
    
    # Testuj modele typu ML z metodami fit/predict
    ml_results = {}
    for model_info in models:
        model_name = model_info['name']
        
        # JeÅ›li podano listÄ™ modeli do testowania, sprawdÅº czy ten model jest na liÅ›cie
        if models_to_test and model_name not in models_to_test:
            continue
            
        instance = model_info.get('instance')
        
        # SprawdÅº czy model ma metody fit i predict
        if instance and model_info.get('has_fit') and model_info.get('has_predict'):
            print(f"â³ Testowanie modelu ML: {model_name}...")
            
            try:
                # Trenuj model
                instance.fit(X_train, y_train)
                
                # OceÅ„ model
                test_result = model_tester.test_model(model_name)
                ml_results[model_name] = test_result
                
                print(f"âœ… Model {model_name} przetestowany pomyÅ›lnie!")
            except Exception as e:
                print(f"âŒ BÅ‚Ä…d podczas testowania modelu {model_name}: {e}")
                ml_results[model_name] = {
                    "success": False,
                    "error": str(e)
                }
    
    # PoÅ‚Ä…cz wyniki
    final_results = {**results, **ml_results}
    
    # Podsumowanie
    success_count = sum(1 for result in final_results.values() if result.get('success', False))
    print(f"\nğŸ“Š Podsumowanie testÃ³w:")
    print(f"   - Przetestowano {len(final_results)} modeli")
    print(f"   - PomyÅ›lnie: {success_count}")
    print(f"   - Niepowodzenia: {len(final_results) - success_count}")
    
    return final_results

def main():
    """GÅ‚Ã³wna funkcja testowa."""
    try:
        # SprawdÅº czy podano argumenty
        if len(sys.argv) > 1:
            models_to_test = sys.argv[1:]
            print(f"ğŸ” Testowanie wybranych modeli: {', '.join(models_to_test)}")
            results = test_models(models_to_test)
        else:
            print("ğŸ” Testowanie wszystkich modeli AI...")
            results = test_models()
        
        # Zapisz wyniki do pliku
        with open('logs/model_test_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print("\nâœ… Wyniki testÃ³w zapisane w logs/model_test_results.json")
        
        # ZwrÃ³Ä‡ kod wyjÅ›cia
        success_count = sum(1 for result in results.values() if result.get('success', False))
        if success_count == len(results):
            return 0  # Wszystkie testy udane
        else:
            return 1  # NiektÃ³re testy siÄ™ nie powiodÅ‚y
            
    except Exception as e:
        logger.error(f"BÅ‚Ä…d podczas testowania modeli: {e}")
        return 2  # BÅ‚Ä…d podczas testowania

if __name__ == "__main__":
    sys.exit(main())
